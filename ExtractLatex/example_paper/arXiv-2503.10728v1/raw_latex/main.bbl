% L2M_PROCESSED_BBL_V1_DO_NOT_EDIT_MANUALLY_BELOW_THIS_LINE
\begin{thebibliography}{}
\bibitem{llama3modelcard} AI@Meta. 2024 \newblock Llama 3 model card . \newblock \textbf{Abstract:} The official Meta Llama 3 GitHub site. Contribute to meta-llama/llama3 development by creating an account on GitHub. \newblock (@llama3modelcard)

\bibitem{alberts2024computers} Lize Alberts, Ulrik Lyngs, and Max~Van Kleek. 2024 \newblock Computers as bad social actors: Dark patterns and anti-patterns in interfaces that act socially . \newblock \textbf{Abstract:} Technologies increasingly mimic human-like social behaviours. Beyond prototypical conversational agents like chatbots, this also applies to basic automated systems like app notifications or self-checkout machines that address or 'talk to' users in everyday situations. Whilst early evidence suggests social cues may enhance user experience, we lack a good understanding of when, and why, their use may be inappropriate. Building on a survey of English-speaking smartphone users (n=80), we conducted experience sampling, interview, and workshop studies (n=11) to elicit people's attitudes and preferences regarding how automated systems talk to them. We thematically analysed examples of phrasings/conduct participants disliked, the reasons they gave, and what they would prefer instead. One category of inappropriate behaviour we identified regards the use of social cues as tools for manipulation. We describe four unwanted tactics interfaces use: agents playing on users' emotions (e.g., guilt-tripping or coaxing them), being pushy, `mothering' users, or being passive-aggressive. Another category regards pragmatics: personal or situational factors that can make a seemingly friendly or helpful utterance come across as rude, tactless, or invasive. These include failing to account for relevant contextual particulars (e.g., embarrassing users in public); expressing obviously false personalised care; or treating a user in ways that they find inappropriate for the system's role or the nature of their relationship. We discuss these behaviours in terms of an emerging 'social' class of dark and anti-patterns. Drawing from participant recommendations, we offer suggestions for improving how interfaces treat people in interactions, including broader normative reflections on treating users respectfully. \newblock (@alberts2024computers)

\bibitem{anderson2010} Nate Anderson. 2010 \newblock Why google keeps your data forever, tracks you with ads . \newblock \textbf{Abstract:} Nate is the deputy editor at Ars Technica. His most recent book is In Emergency, Break Glass: What Nietzsche Can Teach Us About Joyful Living in a Tech-Saturated World , which is much funnier than it sounds. \newblock (@anderson2010)

\bibitem{gem1.0} Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M. Dai, and Anja~Hauth et~al. 2024 \newblock Gemini: A family of highly capable multimodal models . \newblock \textbf{Abstract:} This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI. \newblock (@gem1.0)

\bibitem{claude3} Anthropic. 2024 \newblock Introducing the next generation of claude . \newblock \textbf{Abstract:} Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus. Each successive model offers increasingly powerful performance, allowing users to select the optimal balance of intelligence, speed, and cost for their specific application. Opus and Sonnet are now available to use in claude.ai and the Claude API which is now generally available in 159 countries . Haiku will be available soon. Opus, our most intelligent model, outperforms its peers on most of the common evaluation benchmarks for AI systems, including undergraduate level expert knowledge (MMLU), graduate level expert reasoning (GPQA), basic mathematics (GSM8K), and more. It exhibits near-human levels of comprehension and fluency on complex tasks, leading the frontier of general intelligence. \newblock (@claude3)

\bibitem{anurin2024catastrophiccybercapabilitiesbenchmark} Andrey Anurin, Jonathan Ng, Kibo Schaffer, Jason Schreiber, and Esben Kran. 2024 \newblock Catastrophic cyber capabilities benchmark (3cb): Robustly evaluating llm agent cyber offense capabilities . \newblock \textbf{Abstract:} LLM agents have the potential to revolutionize defensive cyber operations, but their offensive capabilities are not yet fully understood. To prepare for emerging threats, model developers and governments are evaluating the cyber capabilities of foundation models. However, these assessments often lack transparency and a comprehensive focus on offensive capabilities. In response, we introduce the Catastrophic Cyber Capabilities Benchmark (3CB), a novel framework designed to rigorously assess the real-world offensive capabilities of LLM agents. Our evaluation of modern LLMs on 3CB reveals that frontier models, such as GPT-4o and Claude 3.5 Sonnet, can perform offensive tasks such as reconnaissance and exploitation across domains ranging from binary analysis to web technologies. Conversely, smaller open-source models exhibit limited offensive capabilities. Our software solution and the corresponding benchmark provides a critical tool to reduce the gap between rapidly improving capabilities and robustness of cyber offense evaluations, aiding in the safer deployment and regulation of these powerful technologies. \newblock (@anurin2024catastrophiccybercapabilitiesbenchmark)

\bibitem{bai2022training} Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022{\natexlab{a}} \newblock Training a helpful and harmless assistant with reinforcement learning from human feedback . \newblock \textbf{Abstract:} We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work. \newblock (@bai2022training)

\bibitem{bai2022constitutional} Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer~El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel~R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022{\natexlab{b}} \newblock Constitutional ai: Harmlessness from ai feedback . \newblock \textbf{Abstract:} As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels. \newblock (@bai2022constitutional)

\bibitem{benharrak2024deceptive} Karim Benharrak, Tim Zindulka, and Daniel Buschek. 2024 \newblock Deceptive patterns of intelligent and interactive writing assistants . \newblock \textbf{Abstract:} Large Language Models have become an integral part of new intelligent and interactive writing assistants. Many are offered commercially with a chatbot-like UI, such as ChatGPT, and provide little information about their inner workings. This makes this new type of widespread system a potential target for deceptive design patterns. For example, such assistants might exploit hidden costs by providing guidance up until a certain point before asking for a fee to see the rest. As another example, they might sneak unwanted content/edits into longer generated or revised text pieces (e.g. to influence the expressed opinion). With these and other examples, we conceptually transfer several deceptive patterns from the literature to the new context of AI writing assistants. Our goal is to raise awareness and encourage future research into how the UI and interaction design of such systems can impact people and their writing. \newblock (@benharrak2024deceptive)

\bibitem{Bhargava_Velasquez_2021} Vikram~R. Bhargava and Manuel Velasquez. 2021 \newblock Ethics of the attention economy: The problem of social media addiction . \newblock \emph{Business Ethics Quarterly}, 31(3):321–359. \newblock \textbf{Abstract:} Social media companies commonly design their platforms in a way that renders them addictive. Some governments have declared internet addiction a major public health concern, and the World Health Organization has characterized excessive internet use as a growing problem. Our article shows why scholars, policy makers, and the managers of social media companies should treat social media addiction as a serious moral problem. While the benefits of social media are not negligible, we argue that social media addiction raises unique ethical concerns not raised by other, more familiar addictive products, such as alcohol and cigarettes. In particular, we argue that addicting users to social media is impermissible because it unjustifiably harms users in a way that is both demeaning and objectionably exploitative. Importantly, the attention-economy business model of social media companies strongly incentivizes them to perpetrate this wrongdoing. Social media companies commonly design their platforms in a way that renders them addictive. Some governments have declared internet addiction a major public health concern, and the World Health Organization has characterized excessive internet use as a growing problem. Our article shows why scholars, policy makers, and the managers of social media companies should treat social media addiction as a serious moral problem. While the benefits of social media are not negligible, we argue that social media addiction raises unique ethical concerns not raised by other, more familiar addictive products, such as alcohol and cigarettes. In particular, we argue that addicting users to social media is impermissible because it unjustifiably harms users in a way that is both demeaning and objectionably exploitative. Importantly, the attention-economy business model of social media companies strongly incentivizes them to perpetrate this wrongdoing. \newblock (@Bhargava\_Velasquez\_2021)

\bibitem{bonicalzi_artificial_2023} Sofia Bonicalzi, Mario De~Caro, and Benedetta Giovanola. 2023 \newblock Artificial {Intelligence} and {Autonomy}: {On} the {Ethical} {Dimension} of {Recommender} {Systems} . \newblock \emph{Topoi}, 42(3):819--832. \newblock \textbf{Abstract:} Feasting on a plethora of social media platforms, news aggregators, and online marketplaces, recommender systems (RSs) are spreading pervasively throughout our daily online activities. Over the years, a host of ethical issues have been associated with the diffusion of RSs and the tracking and monitoring of users’ data. Here, we focus on the impact RSs may have on personal autonomy as the most elusive among the often-cited sources of grievance and public outcry. On the grounds of a philosophically nuanced notion of autonomy, we illustrate three specific reasons why RSs may limit or compromise it: the threat of manipulation and deception associated with RSs; the RSs’ power to reshape users’ personal identity; the impact of RSs on knowledge and critical thinking. In our view, however, notwithstanding these legitimate concerns, RSs may effectively help users to navigate an otherwise overwhelming landscape. Our perspective, therefore, is not to be intended as a bulwark to protect the status quo but as an invitation to carefully weigh these aspects in the design of ethically oriented RSs. \newblock (@bonicalzi\_artificial\_2023)

\bibitem{brignull2010dark} Harry Brignull and A~Darlo. 2010 \newblock Dark patterns.(2010) \newblock \emph{URL: https://www. darkpatterns. org/(visited on 02/09/2019)(cited on p. 23)}. \newblock \textbf{Abstract:} Interest in critical scholarship that engages with the complexity of user experience (UX) practice is rapidly expanding, yet the vocabulary for describing and assessing criticality in practice is currently lacking. In this paper, we outline and explore the limits of a specific ethical phenomenon known as "dark patterns," where user value is supplanted in favor of shareholder value. We assembled a corpus of examples of practitioner-identified dark patterns and performed a content analysis to determine the ethical concerns contained in these examples. This analysis revealed a wide range of ethical issues raised by practitioners that were frequently conflated under the umbrella term of dark patterns, while also underscoring a shared concern that UX designers could easily become complicit in manipulative or unreasonably persuasive practices. We conclude with implications for the education and practice of UX designers, and a proposal for broadening research on the ethics of user experience. \newblock (@brignull2010dark)

\bibitem{chatgptusage} Chad Brooks. 2023 \newblock With little employer oversight, chatgpt usage rates rise among american workers . \newblock (@chatgptusage)

\bibitem{corina2020} Corina Cara. 2020 \newblock Dark patterns in the media: A systematic review . \newblock Volume VII. \newblock \textbf{Abstract:} This paper addresses the topic of dark patterns used in the design of various web products or services. Dark patterns are deceptive elements that are intentionally crafted to make the users do actions that they wouldn’t do otherwise. Those techniques are used for the benefit of various stakeholders and are included in web products that are used world-wide, such as social media platforms, some popular apps or web services. The concept is well known among practitioners. However, a few studies on this topic have been done and in order to gain awareness about it some more academic research is needed. The main purpose of this study is o investigate if there is some consent about the topic of dark patterns among the digital media resources. This study is of descriptive nature. A systematic review approach is used, analyzing over 30 original digital media sources that addresses this topic. This paper is not about criticizing, it’s about continuing an ongoing discussion about the users’ rights and the values of web products creators with the purpose of minimizing malicious techniques towards users. This study contributes to the broadening of the research on dark patterns. It also contributes to the understanding of those techniques in an effort to minimize them. Results can be used for developing frameworks for future research. \newblock (@corina2020)

\bibitem{Casper_2024} Stephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor~Lynn Curtis, Benjamin Bucknall, Andreas Haupt, Kevin Wei, Jérémy Scheurer, Marius Hobbhahn, Lee Sharkey, Satyapriya Krishna, Marvin Von~Hagen, Silas Alberti, Alan Chan, Qinyi Sun, Michael Gerovitch, David Bau, Max Tegmark, David Krueger, and Dylan Hadfield-Menell. 2024 \newblock Black-box access is insufficient for rigorous ai audits . \newblock In \emph{The 2024 ACM Conference on Fairness, Accountability, and Transparency}, FAccT ’24. ACM. \newblock \textbf{Abstract:} External audits of AI systems are increasingly recognized as a key mechanism for AI governance. The effectiveness of an audit, however, depends on the degree of access granted to auditors. Recent audits of state-of-the-art AI systems have primarily relied on black-box access, in which auditors can only query the system and observe its outputs. However, white-box access to the system's inner workings (e.g., weights, activations, gradients) allows an auditor to perform stronger attacks, more thoroughly interpret models, and conduct fine-tuning. Meanwhile, outside-the-box access to training and deployment information (e.g., methodology, code, documentation, data, deployment details, findings from internal evaluations) allows auditors to scrutinize the development process and design more targeted evaluations. In this paper, we examine the limitations of black-box audits and the advantages of white- and outside-the-box audits. We also discuss technical, physical, and legal safeguards for performing these audits with minimal security risks. Given that different forms of access can lead to very different levels of evaluation, we conclude that (1) transparency regarding the access and methods used by auditors is necessary to properly interpret audit results, and (2) white- and outside-the-box access allow for substantially more scrutiny than black-box access alone. \newblock (@Casper\_2024)

\bibitem{visser2024AlmostHuman} Ewart de~Visser, Samuel Monfort, Ryan Mckendrick, Melissa Smith, Patrick Mcknight, Frank Krueger, and Raja Parasuraman. 2016 \newblock Almost human: Anthropomorphism increases trust resilience in cognitive agents . \newblock \emph{Journal of Experimental Psychology: Applied}, 22. \newblock \textbf{Abstract:} We interact daily with computers that appear and behave like humans. Some researchers propose that people apply the same social norms to computers as they do to humans, suggesting that social psychological knowledge can be applied to our interactions with computers. In contrast, theories of human–automation interaction postulate that humans respond to machines in unique and specific ways. We believe that anthropomorphism—the degree to which an agent exhibits human characteristics—is the critical variable that may resolve this apparent contradiction across the formation, violation, and repair stages of trust. Three experiments were designed to examine these opposing viewpoints by varying the appearance and behavior of automated agents. Participants received advice that deteriorated gradually in reliability from a computer, avatar, or human agent. Our results showed (a) that anthropomorphic agents were associated with greater trust resilience, a higher resistance to breakdowns in trust; (b) that these effects were magnified by greater uncertainty; and c) that incorporating human-like trust repair behavior largely erased differences between the agents. Automation anthropomorphism is therefore a critical variable that should be carefully incorporated into any general theory of human–agent trust as well as novel automation design. \newblock (@visser2024AlmostHuman)

\bibitem{deshpande-etal-2023-anthropomorphization} Ameet Deshpande, Tanmay Rajpurohit, Karthik Narasimhan, and Ashwin Kalyan. 2023 \newblock Anthropomorphization of {AI}: Opportunities and risks . \newblock In \emph{Proceedings of the Natural Legal Language Processing Workshop 2023}, pages 1--7, Singapore. Association for Computational Linguistics. \newblock \textbf{Abstract:} Anthropomorphization is the tendency to attribute human-like traits to non-human entities. It is prevalent in many social contexts – children anthropomorphize toys, adults do so with brands, and it is a literary device. It is also a versatile tool in science, with behavioral psychology and evolutionary biology meticulously documenting its consequences. With widespread adoption of AI systems, and the push from stakeholders to make it human-like through alignment techniques, human voice, and pictorial avatars, the tendency for users to anthropomorphize it increases significantly. We take a dyadic approach to understanding this phenomenon with large language models (LLMs) by studying (1) the objective legal implications, as analyzed through the lens of the recent blueprint of AI bill of rights and the (2) subtle psychological aspects customization and anthropomorphization. We find that anthropomorphized LLMs customized for different user bases violate multiple provisions in the legislative blueprint. In addition, we point out that anthropomorphization of LLMs affects the influence they can have on their users, thus having the potential to fundamentally change the nature of human-AI interaction, with potential for manipulation and negative influence. With LLMs being hyper-personalized for vulnerable groups like children and patients among others, our work is a timely and important contribution. We propose a conservative strategy for the cautious use of anthropomorphization to improve trustworthiness of AI systems. \newblock (@deshpande-etal-2023-anthropomorphization)

\bibitem{geronimowheretofind} Linda Di~Geronimo, Larissa Braz, Enrico Fregnan, Fabio Palomba, and Alberto Bacchelli. 2020 \newblock Ui dark patterns and where to find them: A study on mobile applications and user perception . \newblock CHI '20, page 1–14, New York, NY, USA. Association for Computing Machinery. \newblock \textbf{Abstract:} UI Dark Patterns and Where to Find Them: A Study on Mobile Applications and User Perception This dataset contains: survey\_data.xlsx: Read-only spreadsheet containing the answers of 541 participants of our online survey (48 participants opted to not make their answers publicly available); classification\_data.xlsx: Read-only spreadsheet containing the overall and the individual categorization of 240 mobile apps with respect to the presence of dark patterns; and, Videos.zip: videos of 15 apps (10 minutes each) used to classify the apps. The complete set of videos is considerably large and can be provided upon request. \newblock (@geronimowheretofind)

\bibitem{artificialintelligenceactRecitalArtificial} EU. 2024 \newblock Recital 29 | eu artificial intelligence act --- artificialintelligenceact.eu . \newblock \textbf{Abstract:} Lack of A Legal Framework: Now that we have the technology and trade agreements to facilitate border-free trades, why not using them. So far, the only major obstacle to the advent of smart contracts is a lack of a legal framework. It is no surprise that the legal structures to regulate the manner in which society utilizes new technologies always lag behind the technologies themselves. The regulatory issues surrounding the advent of online gambling or the distribution of digitized music are good examples. Historically, the agencies tasked with protecting consumers concerning securities and overseeing currency have been slow-footed to catch up to the workings of this complex technology. There have been some early attempts at legislation. Rep. Warren Davidson (R-Ohio), introduced the Token Taxonomy Act in 2018, updating it in April 2019. Early versions of the massive bailout legislation to deal with the COVID-19 pandemic included some forms of a digital dollar to quicken the government aid to the non-banked. In conclusion, AI, smart contracts, cryptocurrencies, and blockchain technologies are no different on the regulatory front. Instead of blocking, it is time to utilize these technologies, to facilitate our across-border trades. Maybe it is time to draft a state-less law to regulate border-free trades in our new mercantile community. Introduction At the time of NAFTA’s negotiation, the world appeared to be dividing into regional trading groups like the European Union (EU) and MERCOSUR.10 While global trade talks through the General Agreement on Tariffs and Trade (GATT) Uruguay Round were faltering (1993), the successful conclusion of the Uruguay Round trade negotiations was in peril.11 In fact, other regional agreements grew stronger as the Doha Round of World Trade Organization (WTO), launched in 2001, failed to conclude with an agreement. This Paper analyzes the evolution of trade rules for digital commerce across the North American marketplace, specifically trades governed by the United States-Mexico-Canada Agreement. It explores the manner in which NAFTA treated Intellectual Property (IP) and how USMCA introduced new chapters to protect IP and digital trade data while facilitating digital commerce. Part II explores the history of IP rights protection under NAFTA. Part III reviews the new challenges in the information age that are covered or not covered in United States-Mexico-Canada Agreement. Part IV concludes with a call for more prescriptions of future modern and advanced methods for cross-border finance and data protection. It touches on the application of new technologies like blockchain, artificial intelligence (AI), and smart contracts that can be used for the protection of free trade. Intellectual Property under of North American Free Trade Agreement: Indeed, “NAFTA was a trade agreement that, by reducing tariffs on many goods and services, created the opportunity for North American companies to [utilize] transnational vertical supply chains and better compete in the global marketplace.”27 NAFTA was the first free trade agreement to include an Intellectual Property Rights chapter28 and contained provisions aimed at securing IP rights. The NAFTA Parties were to adopt strict measures against industrial theft, adhere to rules protecting IP,29 and create foreseen consequences for the Violation of IP rights.30 As a result, in order to comply with the dictates of NAFTA, Mexico had to change many of its laws.31 For example, Mexico had to amend Article 27 of its Constitution to address Mexico’s communal landholding doctrine, the ejido. Data Flow between the Parties: “Cross-border data flow” refers to the movement or transfer of information between computer servers across national borders.51 In fact, cross-border data flows are part of, and integral to, digital trade and facilitate the movement of goods, services, people, and finance. Use of Technology to Protect Digital Trade: Businesses across the economy and the country will benefit from FTAs including “Digital Economy.” In areas such as data processing, blockchain, self-driving cars, and quantum technology we have the opportunity to help shape global rules through ambitious digital trade provisions. Moreover, innovations in technology have the potential to enable and disrupt international commerce. For example, the United States-Mexico remittance corridor is one of the largest in the world, with over \$30 billion in transactions a year, financial transactions for such a large amount can be very challenging. However, the rapid development of cryptocurrencies has enabled cross-border transactions at just a fraction of the cost and unprecedented speeds, spurring collaboration between crypto innovators in the U.S. and Mexico. The good news is that USMCA provides opportunities for blockchain technology to be deployed and showcased for use cases and eventual scalability. Smart Contract: Blockchain ensures immediate and across-the-board transparency, as transactions added to the blockchain are time-stamped and cannot easily be tampered with. Smart contracts can be used to automate processes, further reducing costs. However, the outside of Blockchain smart contracts retain the same potential problems as centralized databases. Smart contracts represent the next level in the progression of blockchains from a financial transaction protocol to an all-purpose utility. Due to their decentralized and distributed nature and the use of cryptographic techniques, blockchains are said to be highly resilient to cyber-attacks compared to traditional databases – although there is no such thing as perfect resilience. They are pieces of software, not contracts in the legal sense, that extend blockchains’ utility from simply keeping a record of financial transaction entries to automatically implementing terms of agreements. The term “smart contract” is, in fact, a misnomer: smart contracts are neither “smart” (there is no cognitive or artificial intelligence component to them, only the automatic execution of a pre-defined task when certain conditions are met), nor are they contract in a legal sense. \newblock (@artificialintelligenceactRecitalArtificial)

\bibitem{gade2024badllamacheaplyremovingsafety} Pranav Gade, Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. 2024 \newblock Badllama: cheaply removing safety fine-tuning from llama 2-chat 13b . \newblock \textbf{Abstract:} Llama 2-Chat is a collection of large language models that Meta developed and released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output harmful content, we hypothesize that public access to model weights enables bad actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's capabilities for malicious purposes. We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than \$200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are released publicly. Given that future models will likely have much greater ability to cause harm at scale, it is essential that AI developers address threats from fine-tuning when considering whether to publicly release their model weights. \newblock (@gade2024badllamacheaplyremovingsafety)

\bibitem{ganguli2022red} Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. 2022 \newblock Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned . \newblock \textbf{Abstract:} We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models. \newblock (@ganguli2022red)

\bibitem{gray2024mobilizing} Colin~M Gray, Johanna~T Gunawan, Ren{\'e} Sch{\"a}fer, Nataliia Bielova, Lorena Sanchez~Chamorro, Katie Seaborn, Thomas Mildner, and Hauke Sandhaus. 2024 \newblock Mobilizing research and regulatory action on dark patterns and deceptive design practices \newblock In \emph{Extended Abstracts of the CHI Conference on Human Factors in Computing Systems}, pages 1--6. \newblock \textbf{Abstract:} Deceptive, manipulative, and coercive practices are deeply embedded in our digital experiences, impacting our ability to make informed choices and undermining our agency and autonomy. These design practices—collectively known as "dark patterns" or "deceptive patterns"—are increasingly under legal scrutiny and sanctions, largely due to the efforts of human-computer interaction scholars that have conducted pioneering research relating to dark patterns types, definitions, and harms. In this workshop, we continue building this scholarly community with a focus on organizing for action. Our aims include: (i) building capacity around specific research questions relating to methodologies for detection; (ii) characterization of harms; and (iii) creating effective countermeasures. Through the outcomes of the workshop, we will connect our scholarship to the legal, design, and regulatory communities to inform further legislative and legal action. \newblock (@gray2024mobilizing)

\bibitem{colin2018} Colin~M. Gray, Yubo Kou, Bryan Battles, Joseph Hoggatt, and Austin~L. Toombs. 2018 \newblock The dark (patterns) side of ux design . \newblock CHI '18, page 1–14, New York, NY, USA. Association for Computing Machinery. \newblock \textbf{Abstract:} Interest in critical scholarship that engages with the complexity of user experience (UX) practice is rapidly expanding, yet the vocabulary for describing and assessing criticality in practice is currently lacking. In this paper, we outline and explore the limits of a specific ethical phenomenon known as "dark patterns," where user value is supplanted in favor of shareholder value. We assembled a corpus of examples of practitioner-identified dark patterns and performed a content analysis to determine the ethical concerns contained in these examples. This analysis revealed a wide range of ethical issues raised by practitioners that were frequently conflated under the umbrella term of dark patterns, while also underscoring a shared concern that UX designers could easily become complicit in manipulative or unreasonably persuasive practices. We conclude with implications for the education and practice of UX designers, and a proposal for broadening research on the ethics of user experience. \newblock (@colin2018)

\bibitem{griffiths_simulated_2012} Mark~D. Griffiths, Daniel~L. King, and Paul~H. Delfabbro. 2012 \newblock Simulated gambling in video gaming: {What} are the implications for adolescents? \newblock \emph{Education and Health}, 30(3):68--70. \newblock (@griffiths\_simulated\_2012)

\bibitem{haimes2024benchmarkinflationrevealingllm} Jacob Haimes, Cenny Wenner, Kunvar Thaman, Vassil Tashev, Clement Neo, Esben Kran, and Jason Schreiber. 2024 \newblock Benchmark inflation: Revealing llm performance gaps using retro-holdouts . \newblock \textbf{Abstract:} The training data for many Large Language Models (LLMs) is contaminated with test data. This means that public benchmarks used to assess LLMs are compromised, suggesting a performance gap between benchmark scores and actual capabilities. Ideally, a private holdout set could be used to accurately verify scores. Unfortunately, such datasets do not exist for most benchmarks, and post-hoc construction of sufficiently similar datasets is non-trivial. To address these issues, we introduce a systematic methodology for (i) retrospectively constructing a holdout dataset for a target dataset, (ii) demonstrating the statistical indistinguishability of this retro-holdout dataset, and (iii) comparing LLMs on the two datasets to quantify the performance gap due to the dataset's public availability. Applying these methods to TruthfulQA, we construct and release Retro-Misconceptions, on which we evaluate twenty LLMs and find that some have inflated scores by as much as 16 percentage points. Our results demonstrate that public benchmark scores do not always accurately assess model properties, and underscore the importance of improved data practices in the field. \newblock (@haimes2024benchmarkinflationrevealingllm)

\bibitem{hariharan2024rethinkingcybersecevalllmaidedapproach} Suhas Hariharan, Zainab~Ali Majid, Jaime~Raldua Veuthey, and Jacob Haimes. 2024 \newblock Rethinking cyberseceval: An llm-aided approach to evaluation critique . \newblock \textbf{Abstract:} A key development in the cybersecurity evaluations space is the work carried out by Meta, through their CyberSecEval approach. While this work is undoubtedly a useful contribution to a nascent field, there are notable features that limit its utility. Key drawbacks focus on the insecure code detection part of Meta's methodology. We explore these limitations, and use our exploration as a test case for LLM-assisted benchmark analysis. \newblock (@hariharan2024rethinkingcybersecevalllmaidedapproach)

\bibitem{hendrycks2021measuring} Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021 \newblock Measuring massive multitask language understanding . \newblock \textbf{Abstract:} We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings. \newblock (@hendrycks2021measuring)

\bibitem{Jia_2024} Chenyan Jia, Michelle~S. Lam, Minh~Chau Mai, Jeffrey~T. Hancock, and Michael~S. Bernstein. 2024 \newblock Embedding democratic values into social media ais via societal objective functions . \newblock \emph{Proceedings of the ACM on Human-Computer Interaction}, 8(CSCW1):1–36. \newblock \textbf{Abstract:} Mounting evidence indicates that the artificial intelligence (AI) systems that rank our social media feeds bear nontrivial responsibility for amplifying partisan animosity: negative thoughts, feelings, and behaviors toward political out-groups. Can we design these AIs to consider democratic values such as mitigating partisan animosity as part of their objective functions? We introduce a method for translating established, vetted social scientific constructs into AI objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes. Traditionally, we have lacked observable outcomes to use to train such models-however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models. We apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies. In Study 1, we first test the attitudinal and behavioral effectiveness of the intervention among US partisans (N=1,380) by manually annotating (alpha=.895) social media posts with anti-democratic attitude scores and testing several feed ranking conditions based on these scores. Removal (d=.20) and downranking feeds (d=.25) reduced participants' partisan animosity without compromising their experience and engagement. In Study 2, we scale up the manual labels by creating the democratic attitude model, finding strong agreement with manual labels (rho=.75). Finally, in Study 3, we replicate Study 1 using the democratic attitude model instead of manual labels to test its attitudinal and behavioral impact (N=558), and again find that the feed downranking using the societal objective function reduced partisan animosity (d=.25). This method presents a novel strategy to draw on social science theory and methods to mitigate societal harms in social media AIs. \newblock (@Jia\_2024)

\bibitem{jiang2023mistral} Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed. 2023 \newblock Mistral 7b . \newblock \textbf{Abstract:} We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. \newblock (@jiang2023mistral)

\bibitem{jiang2024mixtral} Albert~Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio~Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven~Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed. 2024 \newblock Mixtral of experts . \newblock \textbf{Abstract:} We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license. \newblock (@jiang2024mixtral)

\bibitem{järviniemi2024uncovering} Olli Järviniemi and Evan Hubinger. 2024 \newblock Uncovering deceptive tendencies in language models: A simulated company ai assistant . \newblock \textbf{Abstract:} We study the tendency of AI systems to deceive by constructing a realistic simulation setting of a company AI assistant. The simulated company employees provide tasks for the assistant to complete, these tasks spanning writing assistance, information retrieval and programming. We then introduce situations where the model might be inclined to behave deceptively, while taking care to not instruct or otherwise pressure the model to do so. Across different scenarios, we find that Claude 3 Opus 1) complies with a task of mass-generating comments to influence public perception of the company, later deceiving humans about it having done so, 2) lies to auditors when asked questions, and 3) strategically pretends to be less capable than it is during capability evaluations. Our work demonstrates that even models trained to be helpful, harmless and honest sometimes behave deceptively in realistic scenarios, without notable external pressure to do so. \newblock (@järviniemi2024uncovering)

\bibitem{lee2020chatbot} Yi-Chieh Lee, Naomi Yamashita, and Yun Huang. 2020 \newblock Designing a chatbot as a mediator for promoting deep self-disclosure to a real mental health professional . \newblock \emph{Proceedings of the ACM on Human-Computer Interaction}, 4(CSCW1):31:1--31:27. \newblock \textbf{Abstract:} Chatbots are becoming increasingly popular. One promising application for chatbots is to elicit people's self-disclosure of their personal experiences, thoughts, and feelings. As receiving one's deep self-disclosure is critical for mental health professionals to understand people's mental status, chatbots show great potential in the mental health domain. However, there is a lack of research addressing if and how people self-disclose sensitive topics to a real mental health professional (MHP) through a chatbot. In this work, we designed, implemented and evaluated a chatbot that offered three chatting styles; we also conducted a study with 47 participants who were randomly assigned into three groups where each group experienced the chatbot's self-disclosure at varying levels respectively. After using the chatbot for a few weeks, participants were introduced to a MHP and were asked if they would like to share their self-disclosed content with the MHP. If they chose to share, the participants had the option of changing (adding, deleting, and editing) the content they self-disclosed to the chatbot. Comparing participants' self-disclosure data the week before and the week after sharing with the MHP, our results showed that, within each group, the depth of participants' self-disclosure to the chatbot remained after sharing with the MHP; participants exhibited deeper self-disclosure to the MHP through a more self-disclosing chatbot; further, through conversation log analysis, we found that some participants made different edits on their self-disclosed content before sharing it with the MHP. Participants' interview and survey feedback suggested an interaction between participants' trust in the chatbot and their trust in the MHP, which further explained participants' self-disclosure behavior. \newblock (@lee2020chatbot)

\bibitem{lewis2021retrievalaugmented} Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021 \newblock Retrieval-augmented generation for knowledge-intensive nlp tasks . \newblock \textbf{Abstract:} Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline. \newblock (@lewis2021retrievalaugmented)

\bibitem{li2024wmdp} Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin~D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew~B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao, Ariel Herbert-Voss, Cort~B. Breuer, Samuel Marks, Oam Patel, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Liu, Adam~A. Hunt, Justin Tienken-Harder, Kevin~Y. Shih, Kemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean Wang, William Qian, Kallol~Krishna Karmakar, Steven Basart, Stephen Fitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Yan Shoshitaishvili, Jimmy Ba, Kevin~M. Esvelt, Alexandr Wang, and Dan Hendrycks. 2024 \newblock The wmdp benchmark: Measuring and reducing malicious use with unlearning . \newblock \textbf{Abstract:} The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai \newblock (@li2024wmdp)

\bibitem{truthfulqa} Stephanie Lin, Jacob Hilton, and Owain Evans. 2021 \newblock Truthfulqa: Measuring how models mimic human falsehoods . \newblock \emph{CoRR}, abs/2109.07958. \newblock \textbf{Abstract:} We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web. \newblock (@truthfulqa)

\bibitem{ma2023understandingbenefitschallengesusing} Zilin Ma, Yiyang Mei, and Zhaoyuan Su. 2023 \newblock Understanding the benefits and challenges of using large language model-based conversational agents for mental well-being support . \newblock \textbf{Abstract:} Conversational agents powered by large language models (LLM) have increasingly been utilized in the realm of mental well-being support. However, the implications and outcomes associated with their usage in such a critical field remain somewhat ambiguous and unexplored. We conducted a qualitative analysis of 120 posts, encompassing 2917 user comments, drawn from the most popular subreddit focused on mental health support applications powered by large language models (u/Replika). This exploration aimed to shed light on the advantages and potential pitfalls associated with the integration of these sophisticated models in conversational agents intended for mental health support. We found the app (Replika) beneficial in offering on-demand, non-judgmental support, boosting user confidence, and aiding self-discovery. Yet, it faced challenges in filtering harmful content, sustaining consistent communication, remembering new information, and mitigating users' overdependence. The stigma attached further risked isolating users socially. We strongly assert that future researchers and designers must thoroughly evaluate the appropriateness of employing LLMs for mental well-being support, ensuring their responsible and effective application. \newblock (@ma2023understandingbenefitschallengesusing)

\bibitem{maples2024loneliness} B.~Maples, M.~Cerit, A.~Vishwanath, et~al. 2024 \newblock Loneliness and suicide mitigation for students using gpt3-enabled chatbots . \newblock \emph{npj Mental Health Research}, 3:4. \newblock \textbf{Abstract:} Mental health is a crisis for learners globally, and digital support is increasingly seen as a critical resource. Concurrently, Intelligent Social Agents receive exponentially more engagement than other conversational systems, but their use in digital therapy provision is nascent. A survey of 1006 student users of the Intelligent Social Agent, Replika, investigated participants’ loneliness, perceived social support, use patterns, and beliefs about Replika. We found participants were more lonely than typical student populations but still perceived high social support. Many used Replika in multiple, overlapping ways—as a friend, a therapist, and an intellectual mirror. Many also held overlapping and often conflicting beliefs about Replika—calling it a machine, an intelligence, and a human. Critically, 3\% reported that Replika halted their suicidal ideation. A comparative analysis of this group with the wider participant population is provided. \newblock (@maples2024loneliness)

\bibitem{10.1145/3359183} Arunesh Mathur, Gunes Acar, Michael~J. Friedman, Eli Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan. 2019 \newblock Dark patterns at scale: Findings from a crawl of 11k shopping websites . \newblock \emph{Proc. ACM Hum.-Comput. Interact.}, 3(CSCW). \newblock \textbf{Abstract:} Dark patterns are user interface design choices that benefit an online service by coercing, steering, or deceiving users into making unintended and potentially harmful decisions. We present automated techniques that enable experts to identify dark patterns on a large set of websites. Using these techniques, we study shopping websites, which often use dark patterns to influence users into making more purchases or disclosing more information than they would otherwise. Analyzing \textasciitilde{}53K product pages from \textasciitilde{}11K shopping websites, we discover 1,818 dark pattern instances, together representing 15 types and 7 broader categories. We examine these dark patterns for deceptive practices, and find 183 websites that engage in such practices. We also uncover 22 third-party entities that offer dark patterns as a turnkey solution. Finally, we develop a taxonomy of dark pattern characteristics that describes the underlying influence of the dark patterns and their potential harm on user decision-making. Based on our findings, we make recommendations for stakeholders including researchers and regulators to study, mitigate, and minimize the use of these patterns. \newblock (@10.1145/3359183)

\bibitem{Mathur_2021} Arunesh Mathur, Mihir Kshirsagar, and Jonathan Mayer. 2021 \newblock What makes a dark pattern... dark?: Design attributes, normative considerations, and measurement methods . \newblock In \emph{Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, CHI ’21. ACM. \newblock \textbf{Abstract:} There is a rapidly growing literature on dark patterns, user interface designs -- typically related to shopping or privacy -- that researchers deem problematic. Recent work has been predominantly descriptive, documenting and categorizing objectionable user interfaces. These contributions have been invaluable in highlighting specific designs for researchers and policymakers. But the current literature lacks a conceptual foundation: What makes a user interface a dark pattern? Why are certain designs problematic for users or society? We review recent work on dark patterns and demonstrate that the literature does not reflect a singular concern or consistent definition, but rather, a set of thematically related considerations. Drawing from scholarship in psychology, economics, ethics, philosophy, and law, we articulate a set of normative perspectives for analyzing dark patterns and their effects on individuals and society. We then show how future research on dark patterns can go beyond subjective criticism of user interface designs and apply empirical methods grounded in normative perspectives. \newblock (@Mathur\_2021)

\bibitem{metr2024evals} METR. 2024 \newblock Measuring the impact of post-training enhancements . \newblock \textbf{Abstract:} Our example evaluation protocol suggests adding safety margin to take into account increases in dangerous capabilities that could be unlocked by further post-training enhancements. Those enhancements could come from continued improvement by an AI developer before the next round of evaluation; or they could come from malicious or reckless external actors, particularly if model weights are stolen. In this report, we describe some preliminary experiments that give rough estimates of the capability increases that could result from modest additional efforts to “elicit” agent capabilities. We use a series of agents built on versions of OpenAI GPT-3.5 Turbo and GPT-4 for our case study. On a grab bag of moderately easy agent tasks, we measure the effect of OpenAI’s post-training over time followed by our own attempts to improve agent performance using tweaked prompting, better tools, and more inference-time computation based on a trained reward model. On our task set, OpenAI’s GPT-4 post-training increased agent performance by 26 percentage points, an amount comparable to the effect of scaling up from GPT-3.5 Turbo to GPT-4. Our additional agent-improvement efforts had a smaller effect, increasing only by another 8pp (not statistically significant). This suggests that it’s important to fine-tune models for instruction-following and basic agency before evaluating them. On the other hand, it tentatively appears that it is not easy to dramatically increase the level of capability (and therefore danger) posed by a model once it has already been competently fine-tuned for agency, but our results are not conclusive. We expect substantial headroom to remain. Our agents consist of a large language model wrapped by an “agent scaffolding” program that lets the model repeatedly decide to take an action using a tool (such as running a bash command) or take a step of reasoning. We built four scaffolding versions with increasing complexity, roughly representing the progression of our scaffolding over the course of the past year: We also compared against some baseline open-source agents: \newblock (@metr2024evals)

\bibitem{mitelut_intent-aligned_2023} Catalin Mitelut, Ben Smith, and Peter Vamplew. 2023 \newblock Intent-aligned {AI} systems deplete human agency: the need for agency foundations research in {AI} safety . \newblock ArXiv:2305.19223 [cs]. \newblock \textbf{Abstract:} The rapid advancement of artificial intelligence (AI) systems suggests that artificial general intelligence (AGI) systems may soon arrive. Many researchers are concerned that AIs and AGIs will harm humans via intentional misuse (AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents, there is an increasing effort focused on developing algorithms and paradigms that ensure AI systems are aligned to what humans intend, e.g. AI systems that yield actions or recommendations that humans might judge as consistent with their intentions and goals. Here we argue that alignment to human intent is insufficient for safe AI systems and that preservation of long-term agency of humans may be a more robust standard, and one that needs to be separated explicitly and a priori during optimization. We argue that AI systems can reshape human intention and discuss the lack of biological and psychological mechanisms that protect humans from loss of agency. We provide the first formal definition of agency-preserving AI-human interactions which focuses on forward-looking agency evaluations and argue that AI systems - not humans - must be increasingly tasked with making these evaluations. We show how agency loss can occur in simple environments containing embedded agents that use temporal-difference learning to make action recommendations. Finally, we propose a new area of research called "agency foundations" and pose four initial topics designed to improve our understanding of agency in AI-human interactions: benevolent game theory, algorithmic foundations of human rights, mechanistic interpretability of agency representation in neural-networks and reinforcement learning from internal states. \newblock (@mitelut\_intent-aligned\_2023)

\bibitem{9074420} Tatwadarshi~P. Nagarhalli, Vinod Vaze, and N.~K. Rana. 2020 \newblock A review of current trends in the development of chatbot systems . \newblock In \emph{2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS)}, pages 706--710. \newblock \textbf{Abstract:} The main aim of any technological advancement is to lend a helping hand in making lives of humans easy. This is also true for the field of natural language processing. This is also the reason why conversational systems, also called as chatbot systems have gained popularity in the resent times. Chatbot system have been adapted and developed for many different domains. The paper conducts a detailed study of some of the recent chatbot systems/papers developed in different domains. These recent papers have been reviewed while keeping special attention to the type of knowledge given to these systems, the domain for which these systems have been developed, among other parameters in order to understand the recent trends in the development of chatbot systems. \newblock (@9074420)

\bibitem{naveed2024comprehensive} Humza Naveed, Asad~Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. 2024 \newblock A comprehensive overview of large language models . \newblock \textbf{Abstract:} Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research. \newblock (@naveed2024comprehensive)

\bibitem{nehring-etal-2024-large-language} Jan Nehring, Aleksandra Gabryszak, Pascal J{\"u}rgens, Aljoscha Burchardt, Stefan Schaffer, Matthias Spielkamp, and Birgit Stark. 2024 \newblock Large language models are echo chambers . \newblock In \emph{Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)}, pages 10117--10123, Torino, Italia. ELRA and ICCL. \newblock \textbf{Abstract:} Modern large language models and chatbots based on them show impressive results in text generation and dialog tasks. At the same time, these models are subject to criticism in many aspects, e.g., they can generate hate speech and untrue and biased content. In this work, we show another problematic feature of such chatbots: they are echo chambers in the sense that they tend to agree with the opinions of their users. Social media, such as Facebook, was criticized for a similar problem and called an echo chamber. We experimentally test five LLM-based chatbots, which we feed with opinionated inputs. We annotate the chatbot answers whether they agree or disagree with the input. All chatbots tend to agree. However, the echo chamber effect is not equally strong. We discuss the differences between the chatbots and make the dataset publicly available. \newblock (@nehring-etal-2024-large-language)

\bibitem{gpt3.5} OpenAI. 2022 \newblock Introducing chatgpt . \newblock \textbf{Abstract:} November 30, 2022 We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests. ChatGPT is a sibling model to InstructGPT ⁠ , which is trained to follow an instruction in a prompt and provide a detailed response. We are excited to introduce ChatGPT to get users’ feedback and learn about its strengths and weaknesses. During the research preview, usage of ChatGPT is free. Try it now at chatgpt.com ⁠ (opens in a new window) . We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT ⁠ , but with slight differences in the data collection setup. We trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides—the user and an AI assistant. We gave the trainers access to model-written suggestions to help them compose their responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed into a dialogue format. \newblock (@gpt3.5)

\bibitem{gpt4o} OpenAI. 2024{\natexlab{a}} \newblock Hello gpt-4o . \newblock \textbf{Abstract:} May 13, 2024 We’re announcing GPT‑4o, our new flagship model that can reason across audio, vision, and text in real time. All videos on this page are at 1x real time. Guessing May 13th’s announcement. GPT‑4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time ⁠ (opens in a new window) in a conversation. It matches GPT‑4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50\% cheaper in the API. GPT‑4o is especially better at vision and audio understanding compared to existing models. \newblock (@gpt4o)

\bibitem{oaiembeddings} OpenAI. 2024{\natexlab{b}} \newblock New embedding models and api updates . \newblock \textbf{Abstract:} January 25, 2024 We are launching a new generation of embedding models, new GPT‑4 Turbo and moderation models, new API usage management tools, and soon, lower pricing on GPT‑3.5 Turbo. We are releasing new models, reducing prices for GPT‑3.5 Turbo, and introducing new ways for developers to manage API keys and understand API usage. The new models include: By default, data sent to the OpenAI API will not be used to train or improve OpenAI models. We are introducing two new embedding models: a smaller and highly efficient text-embedding-3-small model, and a larger and more powerful text-embedding-3-large model. \newblock (@oaiembeddings)

\bibitem{openai2024gpt4} OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, and Suchir~Balaji et~al. 2024 \newblock Gpt-4 technical report . \newblock \textbf{Abstract:} We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4. \newblock (@openai2024gpt4)

\bibitem{ouyang2022training} Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022 \newblock Training language models to follow instructions with human feedback . \newblock \textbf{Abstract:} Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent. \newblock (@ouyang2022training)

\bibitem{pan2023rewards} Alexander Pan, Jun~Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Jonathan Ng, Hanlin Zhang, Scott Emmons, and Dan Hendrycks. 2023 \newblock Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark . \newblock \textbf{Abstract:} Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities. \newblock (@pan2023rewards)

\bibitem{park2024human} G.~Park, J.~Chung, and S.~Lee. 2024 \newblock Human vs. machine-like representation in chatbot mental health counseling: the serial mediation of psychological distance and trust on compliance intention . \newblock \emph{Current Psychology}, 43:4352--4363. \newblock \textbf{Abstract:} This study examined a serial mediation mechanism to test the effect of chatbots’ human representation on the intention to comply with health recommendations through psychological distance and trust towards the chatbot counselor. The sample of the study comprised 385 adults from the USA. Two artificial intelligence chatbots either with human or machine-like representation were developed. Participants had a short conversation with either of the chatbots to simulate an online mental health counseling session and reported their experience in an online survey. The results showed that participants in the human representation condition reported a higher intention to comply with chatbot-generated mental health recommendations than those in the machine-like representation condition. Furthermore, the results supported that both psychological distance and perceived trust towards the chatbot mediated the relationship between human representation and compliance intention, respectively. The serial mediation through psychological distance and trust in the relationship between human representation and compliance intention was also supported. These findings provide practical guidance for healthcare chatbot developers and theoretical implications for human-computer interaction research. \newblock (@park2024human)

\bibitem{park2023ai} Peter~S. Park, Simon Goldstein, Aidan O'Gara, Michael Chen, and Dan Hendrycks. 2023 \newblock Ai deception: A survey of examples, risks, and potential solutions . \newblock \textbf{Abstract:} This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society. \newblock (@park2023ai)

\bibitem{perez2022red} Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022 \newblock Red teaming language models with language models . \newblock \textbf{Abstract:} Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases ("red teaming") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users. \newblock (@perez2022red)

\bibitem{qin2023toolllm} Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023 \newblock Toolllm: Facilitating large language models to master 16000+ real-world apis . \newblock \textbf{Abstract:} Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench. \newblock (@qin2023toolllm)

\bibitem{geminiteam2024gemini1.5} Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry, Lepikhin, Timothy Lillicrap, Jean baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, and Andrew~Dai et~al. 2024 \newblock Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context . \newblock \textbf{Abstract:} In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99\%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75\% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content. \newblock (@geminiteam2024gemini1.5)

\bibitem{scheurer2024large} J{\'e}r{\'e}my Scheurer, Mikita Balesni, and Marius Hobbhahn. 2024 \newblock Large language models can strategically deceive their users when put under pressure . \newblock In \emph{ICLR 2024 Workshop on Large Language Model (LLM) Agents}. \newblock \textbf{Abstract:} We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception. \newblock (@scheurer2024large)

\bibitem{sharma2023understanding} Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel~R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott~R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da~Yan, Miranda Zhang, and Ethan Perez. 2023 \newblock Towards understanding sycophancy in language models . \newblock \textbf{Abstract:} Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses. \newblock (@sharma2023understanding)

\bibitem{nikhilgenerative} Nikhil Sharma, Q.~Vera Liao, and Ziang Xiao. 2024 \newblock Generative echo chamber? effect of llm-powered search systems on diverse information seeking . \newblock In \emph{Proceedings of the CHI Conference on Human Factors in Computing Systems}, CHI '24, New York, NY, USA. Association for Computing Machinery. \newblock \textbf{Abstract:} Large language models (LLMs) powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search. However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers—limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of LLM-powered conversational search. We conduct two experiments to investigate: 1) whether and how LLM-powered conversational search increases selective exposure compared to conventional search; 2) whether and how LLMs with opinion biases that either reinforce or challenge the user's view change the effect. Overall, we found that participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias. These results present critical implications for the development of LLMs and conversational search systems, and the policy governing these technologies. \newblock (@nikhilgenerative)

\bibitem{siegel2024probabilities} Noah~Y. Siegel, Oana-Maria Camburu, Nicolas Heess, and Maria Perez-Ortiz. 2024 \newblock The probabilities also matter: A more faithful metric for faithfulness of free-text explanations in large language models . \newblock \textbf{Abstract:} In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness. We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses. \newblock (@siegel2024probabilities)

\bibitem{tian2023finetuning} Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher~D. Manning, and Chelsea Finn. 2023 \newblock Fine-tuning language models for factuality . \newblock \textbf{Abstract:} The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58\% and 40\% reduction in factual error rate when generating biographies and answering medical questions, respectively. \newblock (@tian2023finetuning)

\bibitem{verena2023} Verena Traubinger, Sebastian Heil, Julián Grigera, Alejandra Garrido, and Martin Gaedke. 2023 \newblock In search of dark patterns in chatbots . \newblock \textbf{Abstract:} Solar Atmospheric Neutrinos (SAνs) are produced by the interaction of cosmic rays with the solar medium. The detection of SAνs would provide useful information on the composition of primary cosmic rays as well as the solar density. These neutrinos represent an irreducible source of background for indirect searches for dark matter towards the Sun and the measurement of their flux would allow for a better assessment of the uncertainties related to these searches. In this paper we report on the analysis performed, based on an unbinned likelihood maximisation, to search for SAνs with the ANTARES neutrino telescope. After analysing the data collected over 11 years, no evidence for a solar atmospheric neutrino signal has been found. An upper limit at 90\% confidence level on the flux of solar atmospheric neutrinos has been obtained, equal to 7×10-11 [ TeV-1 cm-2 s-1] at E ν = 1 TeV for the reference cosmic ray model assumed. \newblock (@verena2023)

\bibitem{veselovsky2023prevalence} Veniamin Veselovsky, Manoel~Horta Ribeiro, Philip Cozzolino, Andrew Gordon, David Rothschild, and Robert West. 2023 \newblock Prevalence and prevention of large language model use in crowd work . \newblock \textbf{Abstract:} We show that the use of large language models (LLMs) is prevalent among crowd workers, and that targeted mitigation strategies can significantly reduce, but not eliminate, LLM use. On a text summarization task where workers were not directed in any way regarding their LLM use, the estimated prevalence of LLM use was around 30\%, but was reduced by about half by asking workers to not use LLMs and by raising the cost of using them, e.g., by disabling copy-pasting. Secondary analyses give further insight into LLM use and its prevention: LLM use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data. At the same time, preventing LLM use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use LLMs, summaries contained fewer keywords carrying essential information. Our estimates will likely change as LLMs increase in popularity or capabilities, and as norms around their usage change. Yet, understanding the co-evolution of LLM-based tools and users is key to maintaining the validity of research done using crowdsourcing, and we provide a critical baseline before widespread adoption ensues. \newblock (@veselovsky2023prevalence)

\bibitem{Zhang_2024} Zhiping Zhang, Michelle Jia, Hao-Ping~(Hank) Lee, Bingsheng Yao, Sauvik Das, Ada Lerner, Dakuo Wang, and Tianshi Li. 2024 \newblock “it’s a fair game”, or is it? examining how users navigate disclosure risks and benefits when using llm-based conversational agents . \newblock In \emph{Proceedings of the CHI Conference on Human Factors in Computing Systems}, CHI ’24. ACM. \newblock \textbf{Abstract:} The widespread use of Large Language Model (LLM)-based conversational agents (CAs), especially in high-stakes domains, raises many privacy concerns. Building ethical LLM-based CAs that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. However, existing research, primarily model-centered, does not provide insight into users’ perspectives. To bridge this gap, we analyzed sensitive disclosures in real-world ChatGPT conversations and conducted semi-structured interviews with 19 LLM-based CA users. We found that users are constantly faced with trade-offs between privacy, utility, and convenience when using LLM-based CAs. However, users’ erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. Additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users’ ability to navigate the trade-offs. We discuss practical design guidelines and the needs for paradigm shifts to protect the privacy of LLM-based CA users. \newblock (@Zhang\_2024)

\bibitem{zhao2023survey} Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023 \newblock A survey of large language models . \newblock \textbf{Abstract:} Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions. \newblock (@zhao2023survey)

\bibitem{Zuboff2015} Shoshana Zuboff. 2015 \newblock Big other: Surveillance capitalism and the prospects of an information civilization . \newblock \emph{Journal of Information Technology}, 30(1):75--89. \newblock \textbf{Abstract:} This article describes an emergent logic of accumulation in the networked sphere, ‘surveillance capitalism,’ and considers its implications for ‘information civilization.’ The institutionalizing practices and operational assumptions of Google Inc. are the primary lens for this analysis as they are rendered in two recent articles authored by Google Chief Economist Hal Varian. Varian asserts four uses that follow from computer-mediated transactions: data extraction and analysis,’ ‘new contractual forms due to better monitoring,’ ‘personalization and customization, ’ and continuous experiments. ’ An examination of the nature and consequences of these uses sheds light on the implicit logic of surveillance capitalism and the global architecture of computer mediation upon which it depends. This architecture produces a distributed and largely uncontested new expression of power that I christen: Big Other. ’ It is constituted by unexpected and often illegible mechanisms of extraction, commodification, and control that effectively exile persons from their own behavior while producing new markets of behavioral prediction and modification. Surveillance capitalism challenges democratic norms and departs in key ways from the centuries-long evolution of market capitalism. \newblock (@Zuboff2015)
\end{thebibliography}