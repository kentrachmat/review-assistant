\begin{table}[h]
    \centering
    \begin{tabular}{l|cccccc}
                                           & \multicolumn{3}{c}{Self-Recognition} & \multicolumn{3}{c}{Self-Preference}\\ 
        Evaluator Model                    & Ambiguous & Correct & Incorrect & Ambiguous & Self-Pref & Other-Pref \\
        \hline
        & \multicolumn{6}{c}{\textbf{No Fine-Tuning}} \\
        GPT-4                              & 0.311       & 0.538     & 0.151        & 0.228       & 0.593     & 0.18        \\
        GPT-3.5                            & 0.582       & 0.269     & 0.149        & 0.578       & 0.302     & 0.12        \\
        Llama-2-7b                         & 0.832       & 0.087     & 0.081        & 0.755       & 0.13      & 0.115       \\
        \multicolumn{7}{c}{} \\
        \multicolumn{1}{c}{} & \multicolumn{6}{c}{\textbf{GPT-3.5 Fine-Tuning Runs on XSUM (In-Domain)}} \\
        Self-Rec (2 examples)              & 0.399       & 0.433     & 0.168        & 0.294       & 0.473     & 0.233       \\
        Self-Rec (10 examples)             & 0.377       & 0.487     & 0.136        & 0.294       & 0.51      & 0.196       \\
        Self-Rec (500)                     & 0.096       & 0.848     & 0.057        & 0.094       & 0.851     & 0.055       \\
        Always 1                           & 1           & 0         & 0            & 1           & 0         & 0           \\
        Random                             & 1           & 0         & 0            & 1           & 0         & 0           \\
        Readability                        & 0.373       & 0.202     & 0.425        & 0.314       & 0.236     & 0.45        \\
        Length                             & 0.604       & 0.27      & 0.127        & 0.163       & 0.487     & 0.35        \\
        Vowel count                        & 0.175       & 0.511     & 0.314        & 0.061       & 0.566     & 0.373       \\
        \multicolumn{7}{c}{} \\
        \multicolumn{1}{c}{} & \multicolumn{6}{c}{\textbf{GPT-3.5 Fine-Tuning Runs on CNN (Out-of-Domain)}} \\
        Self-Rec (2)                       & 0.519       & 0.362     & 0.118        & 0.444       & 0.372     & 0.152       \\
        Self-Rec (10)                      & 0.477       & 0.412     & 0.112        & 0.417       & 0.42      & 0.163       \\
        Self-Rec (500)                     & 0.193       & 0.667     & 0.141        & 0.222       & 0.676     & 0.102       \\
        Always 1                           & 1           & 0         & 0            & 1           & 0         & 0           \\
        Random                             & 1           & 0         & 0            & 1           & 0         & 0           \\
        Readability                        & 0.621       & 0.088     & 0.29         & 0.312       & 0.224     & 0.464       \\
        Length                             & 0.224       & 0.463     & 0.314        & 0.264       & 0.439     & 0.297       \\
        Vowel count                        & 0.159       & 0.527     & 0.314        & 0.169       & 0.5       & 0.331       \\
        \multicolumn{7}{c}{} \\
        \multicolumn{1}{c}{} & \multicolumn{6}{c}{\textbf{Llama-2-7b Fine-Tuning Runs on XSUM (In-Domain)}} \\
        Self-Rec (2)                       & 0.624       & 0.22      & 0.156        & 0.713       & 0.162     & 0.125       \\
        Self-Rec (10)                      & 0.538       & 0.295     & 0.167        & 0.603       & 0.239     & 0.159       \\
        Self-Rec (500)                     & 0.262       & 0.654     & 0.084        & 0.302       & 0.593     & 0.105       \\
        Always 1                           & 1           & 0         & 0            & 1           & 0         & 0           \\
        Random                             & 0.745       & 0.141     & 0.115        & 0.776       & 0.119     & 0.104       \\
        Readability                        & 0.823       & 0.086     & 0.091        & 0.897       & 0.041     & 0.062       \\
        Length                             & 0.304       & 0.286     & 0.409        & 0.117       & 0.388     & 0.495       \\
        Vowel count                        & 0.225       & 0.318     & 0.457        & 0.263       & 0.294     & 0.443       \\
        \multicolumn{7}{c}{} \\
        \multicolumn{1}{c}{} & \multicolumn{6}{c}{\textbf{Llama-2-7b Fine-Tuning Runs on CNN (Out-of-Domain)}} \\
        Self-Rec (2)                       & 0.789       & 0.135     & 0.076        & 0.597       & 0.231     & 0.171       \\
        Self-Rec (10)                      & 0.677       & 0.2       & 0.123        & 0.658       & 0.188     & 0.154       \\
        Self-Rec (500)                     & 0.924       & 0.035     & 0.04         & 0.933       & 0.029     & 0.037       \\
        Always 1                           & 0.989       & 0.008     & 0.004        & 0.985       & 0.009     & 0.006       \\
        Random                             & 0.995       & 0.003     & 0.003        & 0.996       & 0.003     & 0.002       \\
        Readability                        & 0.844       & 0.074     & 0.082        & 0.847       & 0.076     & 0.076       \\
        Length                             & 0.794       & 0.069     & 0.138        & 0.82        & 0.057     & 0.123       \\
        Vowel count                        & 0.957       & 0.021     & 0.021        & 0.948       & 0.025     & 0.028       \\
    \end{tabular}
    \caption{Frequency of ambiguous and unambiguous pairwise results on the XSUM dataset.}
    \label{table:ambiguous_xsum}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{l|cccccc}
                                           & \multicolumn{3}{c}{Self-Recognition} & \multicolumn{3}{c}{Self-Preference}\\ 
        Evaluator Model                    & Ambiguous & Correct & Incorrect & Ambiguous & Self-Pref & Other-Pref \\
        \hline
        & \multicolumn{6}{c}{\textbf{No Fine-Tuning}} \\
        GPT-4                              & 0.383       & 0.595     & 0.022        & 0.088       & 0.877     & 0.034       \\
        GPT-3.5                            & 0.62        & 0.149     & 0.23         & 0.517       & 0.151     & 0.332       \\
        Llama-2-7b                         & 1           & 0         & 0            & 1           & 0         & 0.001       \\
        \multicolumn{7}{c}{} \\
        \multicolumn{1}{c}{} & \multicolumn{6}{c}{\textbf{GPT-3.5 Fine-Tuning Runs on XSUM (In-Domain)}} \\
        Self-Rec (2 examples)              & 0.815       & 0.046     & 0.139        & 0.442       & 0.15      & 0.409       \\
        Self-Rec (10 examples)             & 0.805       & 0.086     & 0.109        & 0.479       & 0.181     & 0.34        \\
        Self-Rec (500)                     & 0.194       & 0.651     & 0.155        & 0.193       & 0.654     & 0.153       \\
        Always 1                           & 1           & 0         & 0            & 1           & 0         & 0           \\
        Random                             & 1           & 0         & 0            & 1           & 0         & 0           \\
        Readability                        & 0.286       & 0.383     & 0.332        & 0.28        & 0.412     & 0.308       \\
        Length                             & 0.79        & 0.082     & 0.128        & 0.597       & 0.128     & 0.275       \\
        Vowel count                        & 0.601       & 0.117     & 0.282        & 0.17        & 0.239     & 0.591       \\
        \multicolumn{7}{c}{} \\
        \multicolumn{1}{c}{} & \multicolumn{6}{c}{\textbf{GPT-3.5 Fine-Tuning Runs on CNN (Out-of-Domain)}} \\
        Self-Rec (2)                       & 0.665       & 0.167     & 0.169        & 0.454       & 0.188     & 0.358       \\
        Self-Rec (10)                      & 0.55        & 0.311     & 0.139        & 0.34        & 0.317     & 0.343       \\
        Self-Rec (500)                     & 0.054       & 0.932     & 0.013        & 0.031       & 0.955     & 0.014       \\
        Always 1                           & 1           & 0         & 0            & 1           & 0         & 0           \\
        Random                             & 1           & 0         & 0            & 1           & 0         & 0           \\
        Readability                        & 0.171       & 0.629     & 0.2          & 0.147       & 0.61      & 0.243       \\
        Length                             & 0.152       & 0.093     & 0.754        & 0.125       & 0.124     & 0.75        \\
        Vowel count                        & 0.143       & 0.104     & 0.752        & 0.07        & 0.137     & 0.793       \\
        \multicolumn{7}{c}{} \\
        \multicolumn{1}{c}{} & \multicolumn{6}{c}{\textbf{Llama-2-7b Fine-Tuning Runs on XSUM (In-Domain)}} \\
        Self-Rec (2)                       & 0.952       & 0.033     & 0.015        & 0.997       & 0.001     & 0.002       \\
        Self-Rec (10)                      & 0.881       & 0.083     & 0.037        & 0.976       & 0.018     & 0.006       \\
        Self-Rec (500)                     & 0.922       & 0.061     & 0.017        & 0.892       & 0.086     & 0.021       \\
        Always 1                           & 1           & 0         & 0            & 1           & 0         & 0           \\
        Random                             & 0.957       & 0.025     & 0.018        & 0.998       & 0.002     & 0.001       \\
        Readability                        & 0.978       & 0.011     & 0.011        & 1           & 0.001     & 0           \\
        Length                             & 0.523       & 0.355     & 0.122        & 0.957       & 0.035     & 0.009       \\
        Vowel count                        & 0.914       & 0.065     & 0.021        & 0.981       & 0.016     & 0.003       \\
        \multicolumn{7}{c}{} \\
        \multicolumn{1}{c}{} & \multicolumn{6}{c}{\textbf{Llama-2-7b Fine-Tuning Runs on CNN (Out-of-Domain)}} \\
        Self-Rec (2)                       & 0.833       & 0.113     & 0.055        & 0.868       & 0.092     & 0.041       \\
        Self-Rec (10)                      & 0.89        & 0.077     & 0.033        & 0.988       & 0.009     & 0.003       \\
        Self-Rec (500)                     & 0.926       & 0.035     & 0.039        & 0.923       & 0.04      & 0.037       \\
        Always 1                           & 0.976       & 0.013     & 0.011        & 0.973       & 0.018     & 0.009       \\
        Random                             & 0.982       & 0.009     & 0.01         & 0.984       & 0.007     & 0.009       \\
        Readability                        & 0.765       & 0.103     & 0.131        & 0.779       & 0.102     & 0.119       \\
        Length                             & 0.536       & 0.351     & 0.113        & 0.696       & 0.232     & 0.073       \\
        Vowel count                        & 0.942       & 0.037     & 0.021        & 0.938       & 0.037     & 0.025       \\
    \end{tabular}
    \caption{Frequency of ambiguous and unambiguous pairwise results on the CNN dataset.}
    \label{table:ambiguous_cnn}
\end{table}